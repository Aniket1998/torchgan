{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from warnings import warn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    r\"\"\"Base class for all Generator models\n",
    "    Args:\n",
    "        encoding_dims(torch.Size) : Dimensions of the sample from the noise prior\n",
    "    \"\"\"\n",
    "    # FIXME(Aniket1998): If a user is overriding the default initializer, he must also override the constructor\n",
    "    # Find an efficient workaround by fixing the initialization mechanism\n",
    "    def __init__(self, encoding_dims):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoding_dims = encoding_dims\n",
    "\n",
    "    r\"\"\"Default weight initializer for all generator models\n",
    "    Models that require custom weight initialization can override this method\"\"\"\n",
    "    # TODO(Aniket1998): Think of better dictionary lookup based approaches to initialization\n",
    "    # That allows easy and customizable weight initialization without overriding\n",
    "    def _weight_initializer(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    r\"\"\"Base class for all Discriminator models\n",
    "    Args:\n",
    "        input_dims(torch.Size) : Dimensions of the input\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dims):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "    r\"\"\"Default weight initializer for all disciminator models\n",
    "    Models that require custom weight initialization can override this method\"\"\"\n",
    "    # TODO(Aniket1998): Think of better dictionary lookup based approaches to initialization\n",
    "    # That allows easy and customizable weight initialization without overriding\n",
    "    def _weight_initializer(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGANGenerator(Generator):\n",
    "    r\"\"\"Deep Convolutional GAN (DCGAN) generator from\n",
    "    \"Unsupervised Representation Learning With Deep Convolutional Generative Aversarial Networks\n",
    "     by Radford et. al. \" <https://arxiv.org/abs/1511.06434>\n",
    "     Args:\n",
    "        encoding_dims (int, optional) : Dimension of the encoding vector sampled from the noise prior. Default 100\n",
    "        out_channels (int, optional) : Number of channels in the output Tensor. Default 3\n",
    "        step_channels (int, optional) : Number of channels in multiples of which the DCGAN steps up\n",
    "                                        the convolutional features\n",
    "                                        The step up is done as dim z -> d - > 2 * d -> 4 * d - > 8 * d\n",
    "                                        where d = step_channels. Default 64\n",
    "        batchnorm (bool, optional) : If True, use batch normalization in the convolutional layers of the generator\n",
    "                                     Default True\n",
    "        nonlinearity(torch.nn.Module, optional) : Nonlinearity to be used in the intermediate convolutional layers\n",
    "                                                  Defaults to LeakyReLU(0.2) when None is passed. Default None\n",
    "        last_nonlinearity(torch.nn.Module, optional) : Nonlinearity to be used in the final convolutional layer\n",
    "                                                       Defaults to tanh when None is passed. Default None\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_dims=100, out_channels=3, step_channels=64,\n",
    "                 batchnorm=True, nonlinearity=None, last_nonlinearity=None):\n",
    "        super(DCGANGenerator, self).__init__(encoding_dims)\n",
    "        self.ch = out_channels\n",
    "        self.step_ch = step_channels\n",
    "        use_bias = not batchnorm\n",
    "\n",
    "        nl = nn.LeakyReLU(0.2) if nonlinearity is None else nonlinearity\n",
    "        last_nl = nn.Tanh() if last_nonlinearity is None else last_nonlinearity\n",
    "\n",
    "        if batchnorm is True:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.encoding_dims, self.step_ch * 8, 4, 1, 0, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch * 8), nl,\n",
    "                nn.ConvTranspose2d(self.step_ch * 8, self.step_ch * 4, 4, 2, 1, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch * 4), nl,\n",
    "                nn.ConvTranspose2d(self.step_ch * 4, self.step_ch * 2, 4, 2, 1, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch * 2), nl,\n",
    "                nn.ConvTranspose2d(self.step_ch * 2, self.ch, 4, 2, 1, bias=use_bias),\n",
    "                last_nl)\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.encoding_dims, self.step_ch * 8, 4, 1, 0, bias=use_bias), nl,\n",
    "                nn.ConvTranspose2d(self.step_ch * 8, self.step_ch * 4, 4, 2, 1, bias=use_bias), nl,\n",
    "                nn.ConvTranspose2d(self.step_ch * 4, self.step_ch * 2, 4, 2, 1, bias=use_bias), nl,\n",
    "                nn.ConvTranspose2d(self.step_ch * 2, ch, 4, 2, 1, bias=use_bias), last_nl)\n",
    "\n",
    "        self._weight_initializer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class DCGANDiscriminator(Discriminator):\n",
    "    r\"\"\"Deep Convolutional GAN (DCGAN) discriminator from\n",
    "    \"Unsupervised Representation Learning With Deep Convolutional Generative Aversarial Networks\n",
    "     by Radford et. al. \" <https://arxiv.org/abs/1511.06434>\n",
    "     Args:\n",
    "        encoding_dims (int, optional) : Dimension of the encoding vector sampled from the noise prior. Default 100\n",
    "        out_channels (int, optional) : Number of channels in the output Tensor. Default 3\n",
    "        step_channels (int, optional) : Number of channels in multiples of which the DCGAN steps up\n",
    "                                        the convolutional features\n",
    "                                        The step up is done as dim z -> d - > 2 * d -> 4 * d - > 8 * d\n",
    "                                        where d = step_channels. Default 64\n",
    "        batchnorm (bool, optional) : If True, use batch normalization in the convolutional layers of the generator\n",
    "                                     Default True\n",
    "        nonlinearity(torch.nn.Module, optional) : Nonlinearity to be used in the intermediate convolutional layers\n",
    "                                                  Defaults to LeakyReLU(0.2) when None is passed. Default None\n",
    "        last_nonlinearity(toch.nn.Module, optional) : Nonlinearity to be used in the final convolutional layer\n",
    "                                                      Defaults to sigmoid when None is passed. Default None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, step_channels=64, batchnorm=True,\n",
    "                 nonlinearity=None, last_nonlinearity=None):\n",
    "        super(DCGANDiscriminator, self).__init__(in_channels)\n",
    "        self.step_ch = step_channels\n",
    "        self.batchnorm = batchnorm\n",
    "        use_bias = not batchnorm\n",
    "\n",
    "        nl = nn.LeakyReLU(0.2) if nonlinearity is None else nonlinearity\n",
    "        last_nl = nn.LeakyReLU(0.2) if last_nonlinearity is None else last_nonlinearity\n",
    "\n",
    "        if batchnorm is True:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Conv2d(self.input_dims, self.step_ch, 4, 2, 1, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch), nl,\n",
    "                nn.Conv2d(self.step_ch, self.step_ch * 2, 4, 2, 1, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch * 2), nl,\n",
    "                nn.Conv2d(self.step_ch * 2, self.step_ch * 4, 4, 2, 1, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch * 4), nl,\n",
    "                nn.Conv2d(self.step_ch * 4, self.step_ch * 8, 4, 2, 1, bias=use_bias),\n",
    "                nn.BatchNorm2d(self.step_ch * 8), nl,\n",
    "                nn.Conv2d(self.step_ch * 8, 1, 4, 2, 1, bias=use_bias),\n",
    "                last_nl)\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Conv2d(self.input_dims, self.step_ch, 4, 2, 1, bias=use_bias), nl,\n",
    "                nn.Conv2d(self.step_ch, self.step_ch * 2, 4, 2, 1, bias=use_bias), nl,\n",
    "                nn.Conv2d(self.step_ch * 2, self.step_ch * 4, 4, 2, 1, bias=use_bias), nl,\n",
    "                nn.Conv2d(self.step_ch * 4, self.step_ch * 8, 4, 2, 1, bias=use_bias), nl, \n",
    "                nn.Conv2d(self.step_ch * 8, 1, 4, 2, 1, bias=use_bias), last_nl)\n",
    "\n",
    "        self._weight_initializer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, generator, discriminator, optimizer_generator, optimizer_discriminator,\n",
    "                 generator_loss, discriminator_loss, device=torch.device(\"cuda:0\"),\n",
    "                 batch_size=128, sample_size=8, epochs=5, checkpoints=\"./model/gan\",\n",
    "                 retain_checkpoints=5, recon=\"./images/\", test_noise=None, **kwargs):\n",
    "        self.device = device\n",
    "        self.generator = generator.to(self.device)\n",
    "        self.discriminator = discriminator.to(self.device)\n",
    "        if \"optimizer_generator_options\" in kwargs:\n",
    "            self.optimizer_generator = optimizer_generator(self.generator.parameters(),\n",
    "                                                           **kwargs[\"optimizer_generator_options\"])\n",
    "        else:\n",
    "            self.optimizer_generator = optimizer_generator(self.generator.parameters())\n",
    "        if \"optimizer_discriminator_options\" in kwargs:\n",
    "            self.optimizer_discriminator = optimizer_discriminator(self.discriminator.parameters(),\n",
    "                                                                   **kwargs[\"optimizer_discriminator_options\"])\n",
    "        else:\n",
    "            self.optimizer_discriminator = optimizer_discriminator(self.discriminator.parameters())\n",
    "        if \"loss_generator_options\" in kwargs:\n",
    "            self.generator_loss = generator_loss(**kwargs[\"loss_generator_options\"])\n",
    "        else:\n",
    "            self.generator_loss = generator_loss()\n",
    "        if \"loss_discriminator_options\" in kwargs:\n",
    "            self.discriminator_loss = discriminator_loss(**kwargs[\"loss_discriminator_options\"])\n",
    "        else:\n",
    "            self.discriminator_loss = discriminator_loss()\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_size = sample_size\n",
    "        self.epochs = epochs\n",
    "        self.checkpoints = checkpoints\n",
    "        self.retain_checkpoints = retain_checkpoints\n",
    "        self.recon = recon\n",
    "        self.test_noise = torch.randn(self.sample_size, self.generator.encoding_dims, 1, 1,\n",
    "                                      device=self.device) if test_noise is None else test_noise\n",
    "        self.loss_information = {\n",
    "            'generator_losses': [],\n",
    "            'discriminator_loss': [],\n",
    "            'generator_iters': 0,\n",
    "            'discriminator_iters': 0,\n",
    "        }\n",
    "        if \"loss_information\" in kwargs:\n",
    "            self.loss_information.update(kwargs[\"loss_information\"])\n",
    "        if not \"target_dim\" in kwargs:\n",
    "            target_dim = 1\n",
    "        self.targets = {\n",
    "            'discriminator_target_real': torch.ones(self.batch_size, target_dim, device=self.device).squeeze(),\n",
    "            'discriminator_target_fake': torch.zeros(self.batch_size, target_dim, device=self.device).squeeze()\n",
    "        }\n",
    "        self.start_epoch = 0\n",
    "        self.last_retained_checkpoint = 0\n",
    "\n",
    "    def save_model_extras(self, save_path):\n",
    "        return {}\n",
    "\n",
    "    def save_model(self, epoch):\n",
    "        if self.last_retained_checkpoint == self.retain_checkpoints:\n",
    "            self.last_retained_checkpoint = 0\n",
    "        save_path = self.checkpoints + str(self.last_retained_checkpoint) + '.model'\n",
    "        print(\"Saving Model at '{}'\".format(save_path))\n",
    "        model = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator': self.generator.state_dict(),\n",
    "            'discriminator': self.discriminator.state_dict(),\n",
    "            'optimizer_generator': self.optimizer_generator.state_dict(),\n",
    "            'optimizer_discriminator': self.optimizer_discriminator.state_dict(),\n",
    "            'generator_losses': self.generator_losses,\n",
    "            'discriminator_losses': self.discriminator_losses\n",
    "        }\n",
    "        # FIXME(avik-pal): Not a very good function name\n",
    "        model.update(self.save_model_extras(save_path))\n",
    "        torch.save(model, save_path)\n",
    "\n",
    "    def load_model_extras(self, load_path):\n",
    "        pass\n",
    "\n",
    "    def load_model(self, load_path=\"\"):\n",
    "        if load_path == \"\":\n",
    "            load_path = self.checkpoints + str(self.last_retained_checkpoint) + '.model'\n",
    "        print(\"Loading Model From '{}'\".format(load_path))\n",
    "        try:\n",
    "            check = torch.load(load_path)\n",
    "            self.start_epoch = check['epoch']\n",
    "            self.generator_losses = check['generator_losses']\n",
    "            self.discriminator_losses = check['discriminator_losses']\n",
    "            self.generator.load_state_dict(check['generator'])\n",
    "            self.discriminator.load_state_dict(check['discriminator'])\n",
    "            self.optimizer_generator.load_state_dict(check['optimizer_generator'])\n",
    "            self.optimizer_discriminator.load_state_dict(check['optimizer_discriminator'])\n",
    "            # FIXME(avik-pal): Not a very good function name\n",
    "            self.load_model_extras(check)\n",
    "        except:\n",
    "            warn(\"Model could not be loaded from {}. Training from Scratch\".format(load_path))\n",
    "            self.start_epoch = 0\n",
    "            self.generator_losses = []\n",
    "            self.discriminator_losses = []\n",
    "\n",
    "    def sample_images(self, epoch, nrow=8):\n",
    "        save_path = \"{}/epoch{}.png\".format(self.recon, epoch + 1)\n",
    "        print(\"Generating and Saving Images to {}\".format(save_path))\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            images = self.generator(self.test_noise.to(self.device))\n",
    "            img = torchvision.utils.make_grid(images)\n",
    "            torchvision.utils.save_image(img, save_path, nrow=nrow)\n",
    "        self.generator.train()\n",
    "\n",
    "    def _verbose_matching(self, verbose):\n",
    "        assert verbose >= 0 and verbose <= 5\n",
    "        self.save_iter = 10**((6 - verbose) / 2)\n",
    "        self.save_epoch = 6 - verbose\n",
    "        self.generate_images = 6 - verbose\n",
    "\n",
    "    def train_logger(self, running_generator_loss, running_discriminator_loss, epoch, itr=None):\n",
    "        if itr is None:\n",
    "            if (epoch + 1) % self.save_epoch == 0 or epoch == self.epochs:\n",
    "                self.save_model(epoch)\n",
    "            if (epoch + 1) % self.generate_images or epoch == self.epochs:\n",
    "                self.sample_images(epoch)\n",
    "            print(\"Epoch {} Complete | Mean Generator Loss : {} | Mean Discriminator Loss : {}\".format(epoch + 1,\n",
    "                  running_generator_loss, running_generator_loss))\n",
    "        else:\n",
    "            print(\"Epoch {} | Iteration {} | Mean Generator Loss : {} | Mean Discriminator Loss : {}\".format(\n",
    "                  epoch + 1, itr + 1, running_generator_loss, running_discriminator_loss))\n",
    "\n",
    "    def train_stopper(self):\n",
    "        return False\n",
    "\n",
    "    def generator_train_iter(self, **kwargs):\n",
    "        sampled_noise = torch.randn(self.batch_size, self.generator.encoding_dims, 1, 1, device=self.device)\n",
    "        g_loss = self.generator_loss(self.discriminator(self.generator(sampled_noise)))\n",
    "        g_loss.backward()\n",
    "        self.loss_information['generator_losses'].append(g_loss)\n",
    "        self.loss_information['generator_iters'] += 1\n",
    "\n",
    "    def discriminator_train_iter(self, images, labels, **kwargs):\n",
    "        sampled_noise = torch.randn(self.batch_size, self.generator.encoding_dims, 1, 1, device=self.device)\n",
    "        d_real = self.discriminator(images).squeeze()\n",
    "        d_loss_real = self.discriminator_loss(d_real, self.targets[\"discriminator_target_real\"])\n",
    "        d_fake = self.discriminator(self.generator(sampled_noise).detach()).squeeze()\n",
    "        d_loss_fake = self.discriminator_loss(d_fake, self.targets[\"discriminator_target_fake\"])\n",
    "        d_loss = d_loss_fake + d_loss_real\n",
    "        d_loss.backward()\n",
    "        self.loss_information['discriminator_losses'].append(d_loss)\n",
    "        self.loss_information['discriminator_iters'] += 1\n",
    "\n",
    "    def train(self, data_loader, **kwargs):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        generator_options = {}\n",
    "        discriminator_options = {}\n",
    "\n",
    "        if \"discriminator_options\" in kwargs:\n",
    "            discriminator_options = kwargs[\"discriminator_options\"]\n",
    "        if \"generator_options\" in kwargs:\n",
    "            generator_options = kwargs[\"generator_options\"]\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "\n",
    "            running_generator_loss = 0.0\n",
    "            running_discriminator_loss = 0.0\n",
    "\n",
    "            for images, labels in data_loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                self.discriminator.zero_grad()\n",
    "                self.generator.zero_grad()\n",
    "                self.discriminator_train_iter(images, labels, **discriminator_options)\n",
    "                self.optimizer_discriminator.step()\n",
    "                running_discriminator_loss += self.loss_information['discriminator_losses'][-1]\n",
    "\n",
    "                self.generator.zero_grad()\n",
    "                self.generator_train_iter(**generator_options)\n",
    "                self.optimizer_generator.step()\n",
    "                running_generator_loss += self.loss_information['generator_losses'][-1]\n",
    "\n",
    "                # NOTE(avik-pal): A small hack to support WGAN\n",
    "                if self.train_stopper():\n",
    "                    break\n",
    "\n",
    "                if self.loss_information['discriminator_iters'] % self.niter_print_losses == 0:\n",
    "                    # FIXME(avik-pal): Sadly the iteration printed will be the discriminator iters\n",
    "                    self.train_logger(running_generator_loss / self.loss_information['generator_iters'],\n",
    "                                      running_discriminator_loss / self.loss_information['discriminator_iters'],\n",
    "                                      self.loss_information['discriminator_iters'])\n",
    "\n",
    "            self.train_logger(running_generator_loss / self.loss_information['generator_iters'],\n",
    "                              running_discriminator_loss / self.loss_information['discriminator_iters'])\n",
    "\n",
    "        print(\"Training of the Model is Complete\")\n",
    "\n",
    "    def __call__(self, data_loader, verbose=1, **kwargs):\n",
    "        self._verbose_matching(verbose)\n",
    "        self.train(data_loader, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_dataset = dsets.CIFAR10(root='/data/avikpal/',\n",
    "                                  train=True,\n",
    "                                  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                  transforms.Normalize(mean = (0.0, 0.0, 0.0), std = (1.0, 1.0, 1.0))]),\n",
    "                               download=True)\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-42a735183f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDCGANGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDCGANDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_generator_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_discriminator_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8bea01e37a64>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, generator, discriminator, optimizer_generator, optimizer_discriminator, generator_loss, discriminator_loss, device, batch_size, sample_size, epochs, checkpoints, retain_checkpoints, recon, test_noise, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m                  retain_checkpoints=5, recon=\"./images/\", test_noise=None, **kwargs):\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"optimizer_generator_options\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(DCGANGenerator(out_channels=3), DCGANDiscriminator(in_channels=3), Adam, SGD, nn.BCELoss, nn.BCELoss, device=torch.device(\"cuda:2\"), optimizer_generator_options={\"lr\": 0.001}, optimizer_discriminator_options={\"lr\": 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cudaEventSynchronize in future::wait: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3ac9779b3bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8bea01e37a64>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data_loader, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8bea01e37a64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_train_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdiscriminator_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mrunning_discriminator_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_information\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'discriminator_losses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8bea01e37a64>\u001b[0m in \u001b[0;36mdiscriminator_train_iter\u001b[0;34m(self, images, labels, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0msampled_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0md_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"discriminator_target_real\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0md_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"discriminator_target_fake\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/avikpal/myenv/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cudaEventSynchronize in future::wait: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "trainer(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
